ollama:
  host: http://ollama:11434  # Ollama service endpoint in Docker
  model: llama3.1  # Default model; change to your downloaded one (e.g., 'gemma2')
  temperature: 0.7  # Controls response creativity (0-1)
tts:
  mode: offline  # 'offline' for Piper, 'online' for ElevenLabs
  elevenlabs_api_key: YOUR_ELEVENLABS_API_KEY_HERE  # Securely add your key; do not commit to Git